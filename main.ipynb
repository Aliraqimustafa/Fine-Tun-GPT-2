{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForCausalLM, TextDataset, DataCollatorForLanguageModeling, Trainer, TrainingArguments\nimport os\ntokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\ntokenizer.add_special_tokens({'pad_token': '[PAD]'})\nmodel = AutoModelForCausalLM.from_pretrained(\"gpt2\")","metadata":{"execution":{"iopub.status.busy":"2023-08-14T07:48:00.688624Z","iopub.execute_input":"2023-08-14T07:48:00.689015Z","iopub.status.idle":"2023-08-14T07:48:05.789486Z","shell.execute_reply.started":"2023-08-14T07:48:00.688983Z","shell.execute_reply":"2023-08-14T07:48:05.788320Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"path = r\"/kaggle/input/datasets-squad-20/Dataset (3).txt\"\ncache_dir = \"./cache_dir\"\nos.makedirs(cache_dir, exist_ok=True)  # Create the cache directory if it doesn't exist\n\ntext_dataset = TextDataset(\n    tokenizer=tokenizer,\n    file_path=path,\n    block_size=128,\n    cache_dir=cache_dir\n)\ndata_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)","metadata":{"execution":{"iopub.status.busy":"2023-08-14T07:48:05.795020Z","iopub.execute_input":"2023-08-14T07:48:05.797666Z","iopub.status.idle":"2023-08-14T07:48:05.965215Z","shell.execute_reply.started":"2023-08-14T07:48:05.797629Z","shell.execute_reply":"2023-08-14T07:48:05.964234Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/data/datasets/language_modeling.py:53: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ü§ó Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"training_args = TrainingArguments(\n    output_dir=\"./output\",\n    overwrite_output_dir=True,\n    num_train_epochs=3,\n    per_device_train_batch_size=4,\n    save_total_limit=2,\n    save_steps=500,\n    logging_dir=\"./logs\",\n    logging_steps=500,\n    learning_rate=1e-4,\n    warmup_steps=500,\n    weight_decay=0.01,\n)\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    data_collator=data_collator,\n    train_dataset=text_dataset,\n)","metadata":{"execution":{"iopub.status.busy":"2023-08-14T07:48:05.967012Z","iopub.execute_input":"2023-08-14T07:48:05.967734Z","iopub.status.idle":"2023-08-14T07:48:06.112754Z","shell.execute_reply.started":"2023-08-14T07:48:05.967694Z","shell.execute_reply":"2023-08-14T07:48:06.111761Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"trainer.train()","metadata":{"execution":{"iopub.status.busy":"2023-08-14T07:48:25.086928Z","iopub.execute_input":"2023-08-14T07:48:25.087285Z","iopub.status.idle":"2023-08-14T08:05:59.008157Z","shell.execute_reply.started":"2023-08-14T07:48:25.087257Z","shell.execute_reply":"2023-08-14T08:05:59.006806Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='3470' max='6870' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [3470/6870 17:30 < 17:10, 3.30 it/s, Epoch 1.51/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>500</td>\n      <td>2.379500</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>2.240600</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>2.214300</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>2.167100</td>\n    </tr>\n    <tr>\n      <td>2500</td>\n      <td>2.088900</td>\n    </tr>\n    <tr>\n      <td>3000</td>\n      <td>1.949200</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"\u001b[31m‚ï≠‚îÄ\u001b[0m\u001b[31m‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\u001b[0m\u001b[31m‚îÄ‚ïÆ\u001b[0m\n\u001b[31m‚îÇ\u001b[0m in \u001b[92m<module>\u001b[0m:\u001b[94m1\u001b[0m                                                                                    \u001b[31m‚îÇ\u001b[0m\n\u001b[31m‚îÇ\u001b[0m                                                                                                  \u001b[31m‚îÇ\u001b[0m\n\u001b[31m‚îÇ\u001b[0m \u001b[31m‚ù± \u001b[0m1 trainer.train()                                                                              \u001b[31m‚îÇ\u001b[0m\n\u001b[31m‚îÇ\u001b[0m   \u001b[2m2 \u001b[0m                                                                                             \u001b[31m‚îÇ\u001b[0m\n\u001b[31m‚îÇ\u001b[0m                                                                                                  \u001b[31m‚îÇ\u001b[0m\n\u001b[31m‚îÇ\u001b[0m \u001b[2;33m/opt/conda/lib/python3.10/site-packages/transformers/\u001b[0m\u001b[1;33mtrainer.py\u001b[0m:\u001b[94m1645\u001b[0m in \u001b[92mtrain\u001b[0m                    \u001b[31m‚îÇ\u001b[0m\n\u001b[31m‚îÇ\u001b[0m                                                                                                  \u001b[31m‚îÇ\u001b[0m\n\u001b[31m‚îÇ\u001b[0m   \u001b[2m1642 \u001b[0m\u001b[2m‚îÇ   ‚îÇ   \u001b[0minner_training_loop = find_executable_batch_size(                                 \u001b[31m‚îÇ\u001b[0m\n\u001b[31m‚îÇ\u001b[0m   \u001b[2m1643 \u001b[0m\u001b[2m‚îÇ   ‚îÇ   ‚îÇ   \u001b[0m\u001b[96mself\u001b[0m._inner_training_loop, \u001b[96mself\u001b[0m._train_batch_size, args.auto_find_batch_size  \u001b[31m‚îÇ\u001b[0m\n\u001b[31m‚îÇ\u001b[0m   \u001b[2m1644 \u001b[0m\u001b[2m‚îÇ   ‚îÇ   \u001b[0m)                                                                                 \u001b[31m‚îÇ\u001b[0m\n\u001b[31m‚îÇ\u001b[0m \u001b[31m‚ù± \u001b[0m1645 \u001b[2m‚îÇ   ‚îÇ   \u001b[0m\u001b[94mreturn\u001b[0m inner_training_loop(                                                       \u001b[31m‚îÇ\u001b[0m\n\u001b[31m‚îÇ\u001b[0m   \u001b[2m1646 \u001b[0m\u001b[2m‚îÇ   ‚îÇ   ‚îÇ   \u001b[0margs=args,                                                                    \u001b[31m‚îÇ\u001b[0m\n\u001b[31m‚îÇ\u001b[0m   \u001b[2m1647 \u001b[0m\u001b[2m‚îÇ   ‚îÇ   ‚îÇ   \u001b[0mresume_from_checkpoint=resume_from_checkpoint,                                \u001b[31m‚îÇ\u001b[0m\n\u001b[31m‚îÇ\u001b[0m   \u001b[2m1648 \u001b[0m\u001b[2m‚îÇ   ‚îÇ   ‚îÇ   \u001b[0mtrial=trial,                                                                  \u001b[31m‚îÇ\u001b[0m\n\u001b[31m‚îÇ\u001b[0m                                                                                                  \u001b[31m‚îÇ\u001b[0m\n\u001b[31m‚îÇ\u001b[0m \u001b[2;33m/opt/conda/lib/python3.10/site-packages/transformers/\u001b[0m\u001b[1;33mtrainer.py\u001b[0m:\u001b[94m1938\u001b[0m in \u001b[92m_inner_training_loop\u001b[0m     \u001b[31m‚îÇ\u001b[0m\n\u001b[31m‚îÇ\u001b[0m                                                                                                  \u001b[31m‚îÇ\u001b[0m\n\u001b[31m‚îÇ\u001b[0m   \u001b[2m1935 \u001b[0m\u001b[2m‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   \u001b[0m\u001b[96mself\u001b[0m.control = \u001b[96mself\u001b[0m.callback_handler.on_step_begin(args, \u001b[96mself\u001b[0m.state,  \u001b[31m‚îÇ\u001b[0m\n\u001b[31m‚îÇ\u001b[0m   \u001b[2m1936 \u001b[0m\u001b[2m‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   \u001b[0m                                                                          \u001b[31m‚îÇ\u001b[0m\n\u001b[31m‚îÇ\u001b[0m   \u001b[2m1937 \u001b[0m\u001b[2m‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   \u001b[0m\u001b[94mwith\u001b[0m \u001b[96mself\u001b[0m.accelerator.accumulate(model):                                  \u001b[31m‚îÇ\u001b[0m\n\u001b[31m‚îÇ\u001b[0m \u001b[31m‚ù± \u001b[0m1938 \u001b[2m‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   \u001b[0mtr_loss_step = \u001b[96mself\u001b[0m.training_step(model, inputs)                      \u001b[31m‚îÇ\u001b[0m\n\u001b[31m‚îÇ\u001b[0m   \u001b[2m1939 \u001b[0m\u001b[2m‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   \u001b[0m                                                                          \u001b[31m‚îÇ\u001b[0m\n\u001b[31m‚îÇ\u001b[0m   \u001b[2m1940 \u001b[0m\u001b[2m‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   \u001b[0m\u001b[94mif\u001b[0m (                                                                      \u001b[31m‚îÇ\u001b[0m\n\u001b[31m‚îÇ\u001b[0m   \u001b[2m1941 \u001b[0m\u001b[2m‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   \u001b[0margs.logging_nan_inf_filter                                           \u001b[31m‚îÇ\u001b[0m\n\u001b[31m‚îÇ\u001b[0m                                                                                                  \u001b[31m‚îÇ\u001b[0m\n\u001b[31m‚îÇ\u001b[0m \u001b[2;33m/opt/conda/lib/python3.10/site-packages/transformers/\u001b[0m\u001b[1;33mtrainer.py\u001b[0m:\u001b[94m2770\u001b[0m in \u001b[92mtraining_step\u001b[0m            \u001b[31m‚îÇ\u001b[0m\n\u001b[31m‚îÇ\u001b[0m                                                                                                  \u001b[31m‚îÇ\u001b[0m\n\u001b[31m‚îÇ\u001b[0m   \u001b[2m2767 \u001b[0m\u001b[2m‚îÇ   ‚îÇ   ‚îÇ   \u001b[0m\u001b[94mwith\u001b[0m amp.scale_loss(loss, \u001b[96mself\u001b[0m.optimizer) \u001b[94mas\u001b[0m scaled_loss:                     \u001b[31m‚îÇ\u001b[0m\n\u001b[31m‚îÇ\u001b[0m   \u001b[2m2768 \u001b[0m\u001b[2m‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   \u001b[0mscaled_loss.backward()                                                    \u001b[31m‚îÇ\u001b[0m\n\u001b[31m‚îÇ\u001b[0m   \u001b[2m2769 \u001b[0m\u001b[2m‚îÇ   ‚îÇ   \u001b[0m\u001b[94melse\u001b[0m:                                                                             \u001b[31m‚îÇ\u001b[0m\n\u001b[31m‚îÇ\u001b[0m \u001b[31m‚ù± \u001b[0m2770 \u001b[2m‚îÇ   ‚îÇ   ‚îÇ   \u001b[0m\u001b[96mself\u001b[0m.accelerator.backward(loss)                                               \u001b[31m‚îÇ\u001b[0m\n\u001b[31m‚îÇ\u001b[0m   \u001b[2m2771 \u001b[0m\u001b[2m‚îÇ   ‚îÇ   \u001b[0m                                                                                  \u001b[31m‚îÇ\u001b[0m\n\u001b[31m‚îÇ\u001b[0m   \u001b[2m2772 \u001b[0m\u001b[2m‚îÇ   ‚îÇ   \u001b[0m\u001b[94mreturn\u001b[0m loss.detach() / \u001b[96mself\u001b[0m.args.gradient_accumulation_steps                      \u001b[31m‚îÇ\u001b[0m\n\u001b[31m‚îÇ\u001b[0m   \u001b[2m2773 \u001b[0m                                                                                          \u001b[31m‚îÇ\u001b[0m\n\u001b[31m‚îÇ\u001b[0m                                                                                                  \u001b[31m‚îÇ\u001b[0m\n\u001b[31m‚îÇ\u001b[0m \u001b[2;33m/opt/conda/lib/python3.10/site-packages/accelerate/\u001b[0m\u001b[1;33maccelerator.py\u001b[0m:\u001b[94m1821\u001b[0m in \u001b[92mbackward\u001b[0m               \u001b[31m‚îÇ\u001b[0m\n\u001b[31m‚îÇ\u001b[0m                                                                                                  \u001b[31m‚îÇ\u001b[0m\n\u001b[31m‚îÇ\u001b[0m   \u001b[2m1818 \u001b[0m\u001b[2m‚îÇ   ‚îÇ   \u001b[0m\u001b[94melif\u001b[0m \u001b[96mself\u001b[0m.scaler \u001b[95mis\u001b[0m \u001b[95mnot\u001b[0m \u001b[94mNone\u001b[0m:                                                     \u001b[31m‚îÇ\u001b[0m\n\u001b[31m‚îÇ\u001b[0m   \u001b[2m1819 \u001b[0m\u001b[2m‚îÇ   ‚îÇ   ‚îÇ   \u001b[0m\u001b[96mself\u001b[0m.scaler.scale(loss).backward(**kwargs)                                    \u001b[31m‚îÇ\u001b[0m\n\u001b[31m‚îÇ\u001b[0m   \u001b[2m1820 \u001b[0m\u001b[2m‚îÇ   ‚îÇ   \u001b[0m\u001b[94melse\u001b[0m:                                                                             \u001b[31m‚îÇ\u001b[0m\n\u001b[31m‚îÇ\u001b[0m \u001b[31m‚ù± \u001b[0m1821 \u001b[2m‚îÇ   ‚îÇ   ‚îÇ   \u001b[0mloss.backward(**kwargs)                                                       \u001b[31m‚îÇ\u001b[0m\n\u001b[31m‚îÇ\u001b[0m   \u001b[2m1822 \u001b[0m\u001b[2m‚îÇ   \u001b[0m                                                                                      \u001b[31m‚îÇ\u001b[0m\n\u001b[31m‚îÇ\u001b[0m   \u001b[2m1823 \u001b[0m\u001b[2m‚îÇ   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92munscale_gradients\u001b[0m(\u001b[96mself\u001b[0m, optimizer=\u001b[94mNone\u001b[0m):                                          \u001b[31m‚îÇ\u001b[0m\n\u001b[31m‚îÇ\u001b[0m   \u001b[2m1824 \u001b[0m\u001b[2;90m‚îÇ   ‚îÇ   \u001b[0m\u001b[33m\"\"\"\u001b[0m                                                                               \u001b[31m‚îÇ\u001b[0m\n\u001b[31m‚îÇ\u001b[0m                                                                                                  \u001b[31m‚îÇ\u001b[0m\n\u001b[31m‚îÇ\u001b[0m \u001b[2;33m/opt/conda/lib/python3.10/site-packages/torch/\u001b[0m\u001b[1;33m_tensor.py\u001b[0m:\u001b[94m487\u001b[0m in \u001b[92mbackward\u001b[0m                         \u001b[31m‚îÇ\u001b[0m\n\u001b[31m‚îÇ\u001b[0m                                                                                                  \u001b[31m‚îÇ\u001b[0m\n\u001b[31m‚îÇ\u001b[0m   \u001b[2m 484 \u001b[0m\u001b[2m‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   \u001b[0mcreate_graph=create_graph,                                                \u001b[31m‚îÇ\u001b[0m\n\u001b[31m‚îÇ\u001b[0m   \u001b[2m 485 \u001b[0m\u001b[2m‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   \u001b[0minputs=inputs,                                                            \u001b[31m‚îÇ\u001b[0m\n\u001b[31m‚îÇ\u001b[0m   \u001b[2m 486 \u001b[0m\u001b[2m‚îÇ   ‚îÇ   ‚îÇ   \u001b[0m)                                                                             \u001b[31m‚îÇ\u001b[0m\n\u001b[31m‚îÇ\u001b[0m \u001b[31m‚ù± \u001b[0m 487 \u001b[2m‚îÇ   ‚îÇ   \u001b[0mtorch.autograd.backward(                                                          \u001b[31m‚îÇ\u001b[0m\n\u001b[31m‚îÇ\u001b[0m   \u001b[2m 488 \u001b[0m\u001b[2m‚îÇ   ‚îÇ   ‚îÇ   \u001b[0m\u001b[96mself\u001b[0m, gradient, retain_graph, create_graph, inputs=inputs                     \u001b[31m‚îÇ\u001b[0m\n\u001b[31m‚îÇ\u001b[0m   \u001b[2m 489 \u001b[0m\u001b[2m‚îÇ   ‚îÇ   \u001b[0m)                                                                                 \u001b[31m‚îÇ\u001b[0m\n\u001b[31m‚îÇ\u001b[0m   \u001b[2m 490 \u001b[0m                                                                                          \u001b[31m‚îÇ\u001b[0m\n\u001b[31m‚îÇ\u001b[0m                                                                                                  \u001b[31m‚îÇ\u001b[0m\n\u001b[31m‚îÇ\u001b[0m \u001b[2;33m/opt/conda/lib/python3.10/site-packages/torch/autograd/\u001b[0m\u001b[1;33m__init__.py\u001b[0m:\u001b[94m200\u001b[0m in \u001b[92mbackward\u001b[0m               \u001b[31m‚îÇ\u001b[0m\n\u001b[31m‚îÇ\u001b[0m                                                                                                  \u001b[31m‚îÇ\u001b[0m\n\u001b[31m‚îÇ\u001b[0m   \u001b[2m197 \u001b[0m\u001b[2m‚îÇ   \u001b[0m\u001b[2m# The reason we repeat same the comment below is that\u001b[0m                                  \u001b[31m‚îÇ\u001b[0m\n\u001b[31m‚îÇ\u001b[0m   \u001b[2m198 \u001b[0m\u001b[2m‚îÇ   \u001b[0m\u001b[2m# some Python versions print out the first line of a multi-line function\u001b[0m               \u001b[31m‚îÇ\u001b[0m\n\u001b[31m‚îÇ\u001b[0m   \u001b[2m199 \u001b[0m\u001b[2m‚îÇ   \u001b[0m\u001b[2m# calls in the traceback and some print out the last line\u001b[0m                              \u001b[31m‚îÇ\u001b[0m\n\u001b[31m‚îÇ\u001b[0m \u001b[31m‚ù± \u001b[0m200 \u001b[2m‚îÇ   \u001b[0mVariable._execution_engine.run_backward(  \u001b[2m# Calls into the C++ engine to run the bac\u001b[0m   \u001b[31m‚îÇ\u001b[0m\n\u001b[31m‚îÇ\u001b[0m   \u001b[2m201 \u001b[0m\u001b[2m‚îÇ   ‚îÇ   \u001b[0mtensors, grad_tensors_, retain_graph, create_graph, inputs,                        \u001b[31m‚îÇ\u001b[0m\n\u001b[31m‚îÇ\u001b[0m   \u001b[2m202 \u001b[0m\u001b[2m‚îÇ   ‚îÇ   \u001b[0mallow_unreachable=\u001b[94mTrue\u001b[0m, accumulate_grad=\u001b[94mTrue\u001b[0m)  \u001b[2m# Calls into the C++ engine to ru\u001b[0m   \u001b[31m‚îÇ\u001b[0m\n\u001b[31m‚îÇ\u001b[0m   \u001b[2m203 \u001b[0m                                                                                           \u001b[31m‚îÇ\u001b[0m\n\u001b[31m‚ï∞‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïØ\u001b[0m\n\u001b[1;91mKeyboardInterrupt\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">‚ï≠‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïÆ</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>                                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span> <span style=\"color: #800000; text-decoration-color: #800000\">‚ù± </span>1 trainer.train()                                                                              <span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2 </span>                                                                                             <span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/opt/conda/lib/python3.10/site-packages/transformers/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">trainer.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1645</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">train</span>                    <span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1642 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">‚îÇ   ‚îÇ   </span>inner_training_loop = find_executable_batch_size(                                 <span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1643 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">‚îÇ   ‚îÇ   ‚îÇ   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._inner_training_loop, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._train_batch_size, args.auto_find_batch_size  <span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1644 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">‚îÇ   ‚îÇ   </span>)                                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span> <span style=\"color: #800000; text-decoration-color: #800000\">‚ù± </span>1645 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">‚îÇ   ‚îÇ   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> inner_training_loop(                                                       <span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1646 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">‚îÇ   ‚îÇ   ‚îÇ   </span>args=args,                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1647 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">‚îÇ   ‚îÇ   ‚îÇ   </span>resume_from_checkpoint=resume_from_checkpoint,                                <span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1648 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">‚îÇ   ‚îÇ   ‚îÇ   </span>trial=trial,                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/opt/conda/lib/python3.10/site-packages/transformers/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">trainer.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1938</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_inner_training_loop</span>     <span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1935 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.control = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.callback_handler.on_step_begin(args, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.state,  <span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1936 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   </span>                                                                          <span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1937 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">with</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.accelerator.accumulate(model):                                  <span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span> <span style=\"color: #800000; text-decoration-color: #800000\">‚ù± </span>1938 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   </span>tr_loss_step = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.training_step(model, inputs)                      <span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1939 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   </span>                                                                          <span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1940 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> (                                                                      <span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1941 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   </span>args.logging_nan_inf_filter                                           <span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/opt/conda/lib/python3.10/site-packages/transformers/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">trainer.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">2770</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">training_step</span>            <span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2767 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">‚îÇ   ‚îÇ   ‚îÇ   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">with</span> amp.scale_loss(loss, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.optimizer) <span style=\"color: #0000ff; text-decoration-color: #0000ff\">as</span> scaled_loss:                     <span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2768 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   </span>scaled_loss.backward()                                                    <span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2769 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">‚îÇ   ‚îÇ   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span>:                                                                             <span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span> <span style=\"color: #800000; text-decoration-color: #800000\">‚ù± </span>2770 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">‚îÇ   ‚îÇ   ‚îÇ   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.accelerator.backward(loss)                                               <span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2771 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">‚îÇ   ‚îÇ   </span>                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2772 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">‚îÇ   ‚îÇ   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> loss.detach() / <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.args.gradient_accumulation_steps                      <span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2773 </span>                                                                                          <span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/opt/conda/lib/python3.10/site-packages/accelerate/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">accelerator.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1821</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">backward</span>               <span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1818 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">‚îÇ   ‚îÇ   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">elif</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.scaler <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">is</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span>:                                                     <span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1819 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">‚îÇ   ‚îÇ   ‚îÇ   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.scaler.scale(loss).backward(**kwargs)                                    <span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1820 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">‚îÇ   ‚îÇ   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span>:                                                                             <span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span> <span style=\"color: #800000; text-decoration-color: #800000\">‚ù± </span>1821 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">‚îÇ   ‚îÇ   ‚îÇ   </span>loss.backward(**kwargs)                                                       <span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1822 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">‚îÇ   </span>                                                                                      <span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1823 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">‚îÇ   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">unscale_gradients</span>(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>, optimizer=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span>):                                          <span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1824 </span><span style=\"color: #bfbfbf; text-decoration-color: #bfbfbf\">‚îÇ   ‚îÇ   </span><span style=\"color: #808000; text-decoration-color: #808000\">\"\"\"</span>                                                                               <span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/opt/conda/lib/python3.10/site-packages/torch/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">_tensor.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">487</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">backward</span>                         <span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 484 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   </span>create_graph=create_graph,                                                <span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 485 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   </span>inputs=inputs,                                                            <span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 486 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">‚îÇ   ‚îÇ   ‚îÇ   </span>)                                                                             <span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span> <span style=\"color: #800000; text-decoration-color: #800000\">‚ù± </span> 487 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">‚îÇ   ‚îÇ   </span>torch.autograd.backward(                                                          <span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 488 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">‚îÇ   ‚îÇ   ‚îÇ   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>, gradient, retain_graph, create_graph, inputs=inputs                     <span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 489 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">‚îÇ   ‚îÇ   </span>)                                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 490 </span>                                                                                          <span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/opt/conda/lib/python3.10/site-packages/torch/autograd/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">__init__.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">200</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">backward</span>               <span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">197 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">‚îÇ   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># The reason we repeat same the comment below is that</span>                                  <span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">198 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">‚îÇ   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># some Python versions print out the first line of a multi-line function</span>               <span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">199 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">‚îÇ   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># calls in the traceback and some print out the last line</span>                              <span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span> <span style=\"color: #800000; text-decoration-color: #800000\">‚ù± </span>200 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">‚îÇ   </span>Variable._execution_engine.run_backward(  <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># Calls into the C++ engine to run the bac</span>   <span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">201 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">‚îÇ   ‚îÇ   </span>tensors, grad_tensors_, retain_graph, create_graph, inputs,                        <span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">202 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">‚îÇ   ‚îÇ   </span>allow_unreachable=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">True</span>, accumulate_grad=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">True</span>)  <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># Calls into the C++ engine to ru</span>   <span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">203 </span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">‚ï∞‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïØ</span>\n<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">KeyboardInterrupt</span>\n</pre>\n"},"metadata":{}}]},{"cell_type":"code","source":"model.save_pretrained(\"./fine_tuned_model\")","metadata":{"execution":{"iopub.status.busy":"2023-08-14T08:06:03.669611Z","iopub.execute_input":"2023-08-14T08:06:03.670162Z","iopub.status.idle":"2023-08-14T08:06:05.041089Z","shell.execute_reply.started":"2023-08-14T08:06:03.670119Z","shell.execute_reply":"2023-08-14T08:06:05.039977Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForCausalLM\n\nmodel_path = \"./fine_tuned_model\"\ntokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\ntokenizer.add_special_tokens({'pad_token': '[PAD]'})\nmodel = AutoModelForCausalLM.from_pretrained(model_path)\n\nmodel.eval()","metadata":{"execution":{"iopub.status.busy":"2023-08-14T08:06:05.043087Z","iopub.execute_input":"2023-08-14T08:06:05.043767Z","iopub.status.idle":"2023-08-14T08:06:07.922173Z","shell.execute_reply.started":"2023-08-14T08:06:05.043732Z","shell.execute_reply":"2023-08-14T08:06:07.920924Z"},"trusted":true},"execution_count":24,"outputs":[{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"GPT2LMHeadModel(\n  (transformer): GPT2Model(\n    (wte): Embedding(50257, 768)\n    (wpe): Embedding(1024, 768)\n    (drop): Dropout(p=0.1, inplace=False)\n    (h): ModuleList(\n      (0-11): 12 x GPT2Block(\n        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (attn): GPT2Attention(\n          (c_attn): Conv1D()\n          (c_proj): Conv1D()\n          (attn_dropout): Dropout(p=0.1, inplace=False)\n          (resid_dropout): Dropout(p=0.1, inplace=False)\n        )\n        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (mlp): GPT2MLP(\n          (c_fc): Conv1D()\n          (c_proj): Conv1D()\n          (act): NewGELUActivation()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n  )\n  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n)"},"metadata":{}}]},{"cell_type":"code","source":"# Generate text using the model\nprompt = \"\"\"[Q] : In which decade did Beyonce become famous?\n[A] :\"\"\"\ninput_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n\n# Generate text\noutput = model.generate(input_ids, max_length=50, num_return_sequences=1, no_repeat_ngram_size=2)\n\n# Decode and print the generated text\ngenerated_text = tokenizer.decode(output[0], skip_special_tokens=True)\nprint(generated_text)","metadata":{"execution":{"iopub.status.busy":"2023-08-14T08:06:09.017599Z","iopub.execute_input":"2023-08-14T08:06:09.017982Z","iopub.status.idle":"2023-08-14T08:06:11.016102Z","shell.execute_reply.started":"2023-08-14T08:06:09.017945Z","shell.execute_reply":"2023-08-14T08:06:11.014981Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"[Q] : In which decade did Beyonce become famous?\n[A] : 1990s\n‚Äôs‚Äùs were the era of the \"Beyonce era\"\n\nBeyce became famous because of her music and her ability\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}